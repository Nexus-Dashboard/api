// routes/migrationRoutes.js
const express = require("express")
const router = express.Router()
const GoogleDriveService = require("../services/googleDriveService")
const { getModel } = require("../config/dbManager")
const { parseDictionarySheet } = require("../services/dictionaryParserService")

let driveService
let serviceInitialized = false

const ensureServiceInitialized = async (req, res, next) => {
  if (serviceInitialized) return next()
  try {
    driveService = new GoogleDriveService()
    await driveService.initialize()
    serviceInitialized = true
    next()
  } catch (error) {
    console.error("Erro ao inicializar servi√ßo Google:", error)
    res.status(500).json({ error: "Erro ao conectar com Google Drive", details: error.message })
  }
}

// Fun√ß√£o para normalizar a vari√°vel (ex: P01 -> P1)
const normalizeVariable = (variable) => {
  if (typeof variable !== "string") return ""
  // Remove o zero √† esquerda se for P01-P09, mas mant√©m P10, P11, etc.
  return variable.replace(/^P0(\d)$/, "P$1")
}

// Fun√ß√£o para introduzir um atraso
const delay = (ms) => new Promise((resolve) => setTimeout(resolve, ms))

// POST /api/migration/sync-index
// Sincroniza o √≠ndice de perguntas para o banco de dados TELEFONICO
router.get("/sync-index", ensureServiceInitialized, async (req, res) => {
  try {
    const QuestionIndex = await getModel("QuestionIndex", "telephonic")
    const indexFileId = "1FurphB54po2Pu-ganTcYqHTMZ7leHuWl_g9hodmhAco"
    console.log(`Iniciando sincroniza√ß√£o do √≠ndice de perguntas para o banco [telephonic]...`)

    const fileData = await driveService.readGoogleSheetsFile(indexFileId)
    const sheet = fileData.sheets["base"]

    if (!sheet || sheet.length < 2) {
      return res.status(400).json({ error: "Planilha de √≠ndice n√£o encontrada ou vazia." })
    }

    const headers = sheet[0].map((h) => h.trim())
    const rows = sheet.slice(1)

    const operations = rows
      .map((row) => {
        const doc = {
          surveyNumber: row[headers.indexOf("N√∫mero da Pesquisa")],
          surveyName: row[headers.indexOf("Arquivo do BD")],
          variable: row[headers.indexOf("Vari√°vel")],
          questionText: row[headers.indexOf("Texto da Pergunta")],
          label: row[headers.indexOf("R√≥tulo")],
          index: row[headers.indexOf("Index")],
          methodology: row[headers.indexOf("Metodologia")],
          map: row[headers.indexOf("Mapa")],
          sample: row[headers.indexOf("Amostra")],
          date: row[headers.indexOf("Data")],
        }
        if (!doc.variable) return null
        return {
          updateOne: {
            filter: { surveyNumber: doc.surveyNumber, variable: doc.variable },
            update: { $set: doc },
            upsert: true,
          },
        }
      })
      .filter(Boolean)

    if (operations.length === 0) {
      return res.status(200).json({ message: "Nenhum dado v√°lido para sincronizar." })
    }

    const result = await QuestionIndex.bulkWrite(operations)
    console.log("Sincroniza√ß√£o do √≠ndice [telephonic] conclu√≠da.")
    res.status(200).json({ message: "√çndice de perguntas [telephonic] sincronizado com sucesso!", ...result })
  } catch (error) {
    console.error("Erro ao sincronizar √≠ndice de perguntas:", error)
    res.status(500).json({ error: "Erro interno no servidor.", details: error.message })
  }
})

// NOVO: Rota para sincronizar o √≠ndice de perguntas F2F
// GET /api/migration/sync-f2f-index
router.get("/sync-f2f-index", ensureServiceInitialized, async (req, res) => {
  try {
    // Conecta ao banco de dados secund√°rio (f2f)
    const QuestionIndex = await getModel("QuestionIndex", "f2f")
    const indexFileId = "1pcJqXSzEzqNYWMdThadgmt3FDib5V5gzZz2DSeXg1AU"
    console.log(`Iniciando sincroniza√ß√£o do √≠ndice de perguntas para o banco [f2f]...`)

    const fileData = await driveService.readGoogleSheetsFile(indexFileId)
    // Assumindo que a aba principal se chama 'base' ou a primeira aba
    const sheetName = fileData.sheetNames[0]
    const sheet = fileData.sheets[sheetName]

    if (!sheet || sheet.length < 2) {
      return res.status(400).json({ error: `Planilha de √≠ndice F2F (aba '${sheetName}') n√£o encontrada ou vazia.` })
    }

    const headers = sheet[0].map((h) => (h ? h.trim() : ""))
    const rows = sheet.slice(1)

    const operations = rows
      .map((row) => {
        const doc = {
          surveyNumber: row[headers.indexOf("N√∫mero da Pesquisa")],
          surveyName: row[headers.indexOf("Arquivo do BD")],
          variable: row[headers.indexOf("Vari√°vel")],
          questionText: row[headers.indexOf("Texto da Pergunta")],
          label: row[headers.indexOf("R√≥tulo")],
          index: row[headers.indexOf("Index")],
          methodology: row[headers.indexOf("Metodologia")],
          map: row[headers.indexOf("Mapa")],
          sample: row[headers.indexOf("Amostra")],
          date: row[headers.indexOf("Data")],
        }
        if (!doc.variable || !doc.surveyNumber) return null
        return {
          updateOne: {
            filter: { surveyNumber: doc.surveyNumber, variable: doc.variable },
            update: { $set: doc },
            upsert: true,
          },
        }
      })
      .filter(Boolean)

    if (operations.length === 0) {
      return res.status(200).json({ message: "Nenhum dado v√°lido para sincronizar no √≠ndice F2F." })
    }

    const result = await QuestionIndex.bulkWrite(operations)
    console.log("Sincroniza√ß√£o do √≠ndice [f2f] conclu√≠da.")
    res.status(200).json({ message: "√çndice de perguntas [f2f] sincronizado com sucesso!", ...result })
  } catch (error) {
    console.error("Erro ao sincronizar √≠ndice de perguntas F2F:", error)
    res.status(500).json({ error: "Erro interno no servidor.", details: error.message })
  }
})

// POST /api/migration/sync-index-answers
// Sincroniza as respostas poss√≠veis do dicion√°rio para o √≠ndice de perguntas TELEFONICO
router.get("/sync-index-answers", ensureServiceInitialized, async (req, res) => {
  try {
    console.log("Iniciando sincroniza√ß√£o de respostas para o √≠ndice [telephonic]...")

    // 1. Fetch and parse all dictionaries SEQUENTIALLY to avoid quota limits
    console.log("Buscando e processando dicion√°rios (sequencialmente)...")
    const dictionaryFilesMap = await driveService.listAllDictionaryFiles() // { rodada: fileId }
    const dictionariesByRodada = {}

    const rodadas = Object.keys(dictionaryFilesMap)
    for (const rodada of rodadas) {
      const fileId = dictionaryFilesMap[rodada]
      console.log(`Processando dicion√°rio da Rodada ${rodada} (ID: ${fileId})`)
      try {
        const dictionaryData = await driveService.readGoogleSheetsFile(fileId)
        if (dictionaryData.sheets["Sheet1"]) {
          dictionariesByRodada[rodada] = parseDictionarySheet(dictionaryData)
          console.log(`  -> Dicion√°rio da Rodada ${rodada} processado com sucesso.`)
        } else {
          console.warn(`‚ö†Ô∏è  Aba 'Sheet1' n√£o encontrada no dicion√°rio da Rodada ${rodada}`)
        }
      } catch (e) {
        console.error(`Erro ao processar dicion√°rio da Rodada ${rodada}:`, e.message)
      }
      // Adiciona um atraso de 1.5 segundos entre cada requisi√ß√£o para n√£o exceder a cota
      await delay(1500)
    }
    console.log(`Total de ${Object.keys(dictionariesByRodada).length} dicion√°rios processados.`)

    // 2. Get the model and fetch all questions
    const QuestionIndex = await getModel("QuestionIndex", "telephonic")
    console.log("Buscando todas as perguntas do √≠ndice [telephonic]...")
    const allQuestions = await QuestionIndex.find({}).lean()
    console.log(`Encontradas ${allQuestions.length} perguntas para atualizar.`)

    // 3. Prepare bulk update operations with normalized variable keys
    const operations = allQuestions
      .map((question) => {
        const surveyNumber = question.surveyNumber
        // Normaliza a vari√°vel da pergunta (ex: P01 -> P1)
        const normalizedVar = normalizeVariable(question.variable)
        let possibleAnswers = []

        if (surveyNumber && normalizedVar && dictionariesByRodada[surveyNumber]) {
          const dictionaryForRodada = dictionariesByRodada[surveyNumber]
          // Tenta encontrar a vari√°vel normalizada no dicion√°rio
          if (dictionaryForRodada[normalizedVar]) {
            possibleAnswers = dictionaryForRodada[normalizedVar]
          }
        }

        // S√≥ atualiza se encontrou novas respostas e antes n√£o tinha nenhuma
        if (possibleAnswers.length > 0 && (!question.possibleAnswers || question.possibleAnswers.length === 0)) {
          return {
            updateOne: {
              filter: { _id: question._id },
              update: { $set: { possibleAnswers: possibleAnswers } },
            },
          }
        }
        return null // N√£o faz nada se n√£o encontrou respostas ou se j√° tinha
      })
      .filter(Boolean)

    if (operations.length === 0) {
      return res.status(200).json({
        message: "Nenhuma pergunta nova para atualizar com respostas. A sincroniza√ß√£o pode j√° estar completa.",
        updatedCount: 0,
      })
    }

    // 4. Execute bulk write
    console.log(`Preparando para atualizar ${operations.length} documentos com novas respostas...`)
    const result = await QuestionIndex.bulkWrite(operations)
    console.log("Sincroniza√ß√£o de respostas do √≠ndice [telephonic] conclu√≠da.")
    res.status(200).json({
      message: "Respostas do √≠ndice [telephonic] sincronizadas com sucesso!",
      updatedCount: result.modifiedCount,
    })
  } catch (error) {
    console.error("Erro ao sincronizar respostas do √≠ndice:", error)
    res.status(500).json({ error: "Erro interno no servidor.", details: error.message })
  }
})

// POST /api/migration/update-variables
// Atualiza o campo variable no QuestionIndex TELEFONICO com os valores da planilha
router.get("/update-variables", ensureServiceInitialized, async (req, res) => {
  try {
    console.log("Iniciando atualiza√ß√£o de vari√°veis no √≠ndice [telephonic]...")

    const QuestionIndex = await getModel("QuestionIndex", "telephonic")
    const indexFileId = "1FurphB54po2Pu-ganTcYqHTMZ7leHuWl_g9hodmhAco"

    console.log("Lendo arquivo de √≠ndice atualizado...")
    const fileData = await driveService.readGoogleSheetsFile(indexFileId)
    const sheet = fileData.sheets["base"]

    if (!sheet || sheet.length < 2) {
      return res.status(400).json({ error: "Planilha de √≠ndice n√£o encontrada ou vazia." })
    }

    const headers = sheet[0].map((h) => h.trim())
    const rows = sheet.slice(1)

    console.log(`Processando ${rows.length} linhas da planilha...`)

    // Preparar opera√ß√µes em lote usando bulkWrite para melhor performance
    const operations = rows
      .map((row) => {
        const surveyNumber = row[headers.indexOf("N√∫mero da Pesquisa")]
        const variable = row[headers.indexOf("Vari√°vel")]
        const surveyName = row[headers.indexOf("Arquivo do BD")]
        const questionText = row[headers.indexOf("Texto da Pergunta")]
        const label = row[headers.indexOf("R√≥tulo")]
        const index = row[headers.indexOf("Index")]
        const methodology = row[headers.indexOf("Metodologia")]
        const map = row[headers.indexOf("Mapa")]
        const sample = row[headers.indexOf("Amostra")]
        const date = row[headers.indexOf("Data")]

        if (!surveyNumber || !variable) {
          return null
        }

        return {
          updateOne: {
            filter: {
              surveyNumber: surveyNumber.toString(),
              variable: variable.trim(),
            },
            update: {
              $set: {
                surveyNumber: surveyNumber.toString(),
                surveyName: surveyName || "",
                variable: variable.trim(),
                questionText: questionText || "",
                label: label || "",
                index: index || "",
                methodology: methodology || "",
                map: map || "",
                sample: sample || "",
                date: date || "",
              },
            },
            upsert: true, // Cria se n√£o existir, atualiza se existir
          },
        }
      })
      .filter(Boolean)

    if (operations.length === 0) {
      return res.status(200).json({ message: "Nenhum dado v√°lido para processar." })
    }

    console.log(`Executando ${operations.length} opera√ß√µes em lote...`)
    const result = await QuestionIndex.bulkWrite(operations, { ordered: false })

    console.log("Atualiza√ß√£o de vari√°veis [telephonic] conclu√≠da.")
    res.status(200).json({
      message: "Vari√°veis [telephonic] atualizadas com sucesso!",
      upsertedCount: result.upsertedCount,
      modifiedCount: result.modifiedCount,
      matchedCount: result.matchedCount,
      totalProcessed: operations.length,
      insertedCount: result.insertedCount || 0,
    })
  } catch (error) {
    console.error("Erro ao atualizar vari√°veis:", error)
    res.status(500).json({ error: "Erro interno no servidor.", details: error.message })
  }
})

// POST /api/migration/sync-surveys
// Migra dados TELEFONICOS do Google Drive para o banco principal
router.get("/sync-surveys", ensureServiceInitialized, async (req, res) => {
  try {
    console.log("Iniciando migra√ß√£o de dados de pesquisas [telephonic]...")

    const allFilesByYear = await driveService.listAllSurveyFiles()
    let totalFilesProcessed = 0
    let totalResponsesMigrated = 0

    const yearsToProcess = Object.keys(allFilesByYear.years)

    for (const year of yearsToProcess) {
      const yearData = allFilesByYear.years[year]
      console.log(`Processando ano: ${year} (${yearData.files.length} arquivos)`)

      // Seleciona os modelos do banco principal
      const Survey = await getModel("Survey", "telephonic")
      const Response = await getModel("Response", "telephonic")

      for (const file of yearData.files) {
        const fileHash = `${file.id}-${file.modifiedTime}`
        const existingSurvey = await Survey.findOne({ fileHashes: fileHash })
        if (existingSurvey) {
          console.log(`  - Arquivo ${file.name} j√° processado. Pulando.`)
          continue
        }

        console.log(`  + Processando arquivo: ${file.name}`)
        const fileData = await driveService.readGoogleSheetsFile(file.id)

        for (const sheetName of Object.keys(fileData.sheets)) {
          const sheetData = fileData.sheets[sheetName]
          if (!sheetData || sheetData.length < 2) continue

          const headers = sheetData[0].map((h) => (h ? h.toString().toUpperCase() : ""))
          const dataRows = sheetData.slice(1)

          if (headers.length === 0 || dataRows.length === 0) continue

          const surveyName = `${file.name} - ${sheetName}`
          const survey = await Survey.findOneAndUpdate(
            { name: surveyName },
            { $set: { year: year, month: file.rodada } },
            { upsert: true, new: true },
          )

          const responses = dataRows.map((row, index) => {
            const entrevistadoId = row[0] || `resp_${index + 1}`
            const answers = headers
              .map((key, index) => {
                const value = row[index]
                if (!key || value === null || value === undefined || value === "") return null
                return { k: key, v: value }
              })
              .filter(Boolean)

            return {
              surveyId: survey._id,
              entrevistadoId: entrevistadoId.toString(),
              answers,
              rodada: file.rodada,
              year: Number.parseInt(year, 10),
            }
          })

          if (responses.length > 0) {
            await Response.insertMany(responses, { ordered: false, lean: true })
            totalResponsesMigrated += responses.length
          }

          await Survey.updateOne({ _id: survey._id }, { $addToSet: { fileHashes: fileHash } })
        }
        totalFilesProcessed++
      }
    }

    const message = "Migra√ß√£o [telephonic] conclu√≠da com sucesso!"
    console.log(message)
    res.status(200).json({
      message,
      filesProcessed: totalFilesProcessed,
      responsesMigrated: totalResponsesMigrated,
    })
  } catch (error) {
    console.error("Erro durante a migra√ß√£o [telephonic]:", error)
    res.status(500).json({ error: "Erro interno no servidor durante a migra√ß√£o.", details: error.message })
  }
})

// NOVO: Rota para migrar dados F2F para o banco secund√°rio COM FILTROS DE TAMANHO
// GET /api/migration/sync-f2f-surveys?skipLargeFiles=true&maxResponses=15000&skipRounds=03,06,08,09
router.get("/sync-f2f-surveys", ensureServiceInitialized, async (req, res) => {
  try {
    const { skipLargeFiles = "true", maxResponses = "15000", skipRounds = "03,06,08,09", dryRun = "false" } = req.query

    console.log("üîç Iniciando migra√ß√£o de dados de pesquisas [f2f] com filtros...")
    console.log(`üìä Configura√ß√µes:`)
    console.log(`   - Pular arquivos grandes: ${skipLargeFiles}`)
    console.log(`   - M√°ximo de respostas por arquivo: ${maxResponses}`)
    console.log(`   - Rodadas a pular: ${skipRounds}`)
    console.log(`   - Modo simula√ß√£o: ${dryRun}`)

    const allFilesByYear = await driveService.listAllF2FSurveyFiles()
    let totalFilesProcessed = 0
    let totalFilesSkipped = 0
    let totalResponsesMigrated = 0
    const skippedFiles = []

    const yearsToProcess = Object.keys(allFilesByYear.years)
    const roundsToSkip = skipRounds ? skipRounds.split(",").map((r) => r.trim().padStart(2, "0")) : []
    const maxResponsesLimit = Number.parseInt(maxResponses)

    console.log(`üö´ Rodadas que ser√£o puladas: ${roundsToSkip.join(", ")}`)

    for (const year of yearsToProcess) {
      const yearData = allFilesByYear.years[year]
      console.log(`üìÖ Processando ano F2F: ${year} (${yearData.files.length} arquivos)`)

      // Seleciona os modelos do banco secund√°rio (f2f)
      const Survey = await getModel("Survey", "f2f")
      const Response = await getModel("Response", "f2f")

      for (const file of yearData.files) {
        const rodadaNumber = file.rodada ? file.rodada.toString().padStart(2, "0") : null
        try {
          // Verificar se deve pular por rodada
          if (rodadaNumber && roundsToSkip.includes(rodadaNumber)) {
            console.log(`üö´ Pulando arquivo por rodada restrita: ${file.name} (Rodada ${rodadaNumber})`)
            totalFilesSkipped++
            skippedFiles.push({
              name: file.name,
              reason: `Rodada ${rodadaNumber} est√° na lista de exclus√£o`,
              year: year,
              rodada: rodadaNumber,
            })
            continue
          }

          const fileHash = `${file.id}-${file.modifiedTime}`
          const existingSurvey = await Survey.findOne({ fileHashes: fileHash })
          if (existingSurvey) {
            console.log(`‚úÖ Arquivo F2F ${file.name} j√° processado. Pulando.`)
            continue
          }

          console.log(`üîç Analisando arquivo F2F: ${file.name}`)

          // Ler o arquivo para verificar o tamanho
          const fileData = await driveService.readGoogleSheetsFile(file.id)

          // Contar total de linhas em todas as abas
          let totalRows = 0
          for (const sheetName of Object.keys(fileData.sheets)) {
            const sheetData = fileData.sheets[sheetName]
            if (sheetData && sheetData.length > 1) {
              // -1 para descontar o header
              totalRows += sheetData.length - 1
            }
          }

          // Verificar se deve pular por tamanho
          if (skipLargeFiles === "true" && totalRows > maxResponsesLimit) {
            console.log(`üö´ Pulando arquivo muito grande: ${file.name}`)
            console.log(`   üìä Respostas encontradas: ${totalRows.toLocaleString()}`)
            console.log(`   üìä Limite configurado: ${maxResponsesLimit.toLocaleString()}`)

            totalFilesSkipped++
            skippedFiles.push({
              name: file.name,
              reason: `Arquivo muito grande (${totalRows.toLocaleString()} respostas > ${maxResponsesLimit.toLocaleString()})`,
              year: year,
              rodada: rodadaNumber,
              totalRows: totalRows,
            })
            continue
          }

          // Se chegou at√© aqui, o arquivo ser√° processado
          console.log(`‚úÖ Arquivo aprovado para migra√ß√£o: ${file.name}`)
          console.log(`   üìä Total de respostas: ${totalRows.toLocaleString()}`)

          // Se for dry run, apenas simular
          if (dryRun === "true") {
            console.log(`üîÑ [SIMULA√á√ÉO] Processaria arquivo: ${file.name}`)
            totalFilesProcessed++
            totalResponsesMigrated += totalRows
            continue
          }

          // Processar o arquivo normalmente
          for (const sheetName of Object.keys(fileData.sheets)) {
            const sheetData = fileData.sheets[sheetName]
            if (!sheetData || sheetData.length < 2) continue

            const headers = sheetData[0].map((h) => (h ? h.toString().toUpperCase() : ""))
            const dataRows = sheetData.slice(1)

            if (headers.length === 0 || dataRows.length === 0) continue

            const surveyName = `${file.name} - ${sheetName}`
            const survey = await Survey.findOneAndUpdate(
              { name: surveyName },
              { $set: { year: year, month: file.rodada } },
              { upsert: true, new: true },
            )

            const responses = dataRows.map((row, index) => {
              const entrevistadoId = row[0] || `resp_${index + 1}`
              const answers = headers
                .map((key, index) => {
                  const value = row[index]
                  if (!key || value === null || value === undefined || value === "") return null
                  return { k: key, v: value }
                })
                .filter(Boolean)

              return {
                surveyId: survey._id,
                entrevistadoId: entrevistadoId.toString(),
                answers,
                rodada: file.rodada,
                year: Number.parseInt(year, 10),
              }
            })

            if (responses.length > 0) {
              // Inserir em lotes menores para evitar problemas de mem√≥ria
              const batchSize = 1000
              for (let i = 0; i < responses.length; i += batchSize) {
                const batch = responses.slice(i, i + batchSize)
                await Response.insertMany(batch, { ordered: false, lean: true })
                console.log(`   üìù Inserido lote ${Math.floor(i / batchSize) + 1}: ${batch.length} respostas`)
              }
              totalResponsesMigrated += responses.length
            }

            await Survey.updateOne({ _id: survey._id }, { $addToSet: { fileHashes: fileHash } })
          }

          totalFilesProcessed++
          console.log(`‚úÖ Arquivo processado com sucesso: ${file.name}`)
        } catch (fileError) {
          console.error(`‚ùå Erro ao processar arquivo ${file.name}:`, fileError.message)

          // Se for erro de quota, parar a migra√ß√£o
          if (fileError.message.includes("space quota") || fileError.message.includes("AtlasError")) {
            console.error(`üö´ ERRO DE QUOTA DETECTADO! Parando migra√ß√£o para evitar mais problemas.`)

            return res.status(507).json({
              success: false,
              error: "Quota de espa√ßo excedida durante a migra√ß√£o",
              message: "Migra√ß√£o interrompida para evitar mais problemas de espa√ßo",
              progress: {
                filesProcessed: totalFilesProcessed,
                filesSkipped: totalFilesSkipped,
                responsesMigrated: totalResponsesMigrated,
                lastProcessedFile: file.name,
              },
              skippedFiles: skippedFiles,
              recommendation: "Considere aumentar o limite de espa√ßo ou pular mais arquivos grandes",
            })
          }

          totalFilesSkipped++
          skippedFiles.push({
            name: file.name,
            reason: `Erro durante processamento: ${fileError.message}`,
            year: year,
            rodada: rodadaNumber,
          })
        }
      }
    }

    const message =
      dryRun === "true" ? "Simula√ß√£o de migra√ß√£o [f2f] conclu√≠da!" : "Migra√ß√£o [f2f] conclu√≠da com sucesso!"

    console.log(message)
    console.log(`üìä Estat√≠sticas finais:`)
    console.log(`   ‚úÖ Arquivos processados: ${totalFilesProcessed}`)
    console.log(`   üö´ Arquivos pulados: ${totalFilesSkipped}`)
    console.log(`   üìù Respostas migradas: ${totalResponsesMigrated.toLocaleString()}`)

    res.status(200).json({
      success: true,
      message,
      isDryRun: dryRun === "true",
      statistics: {
        filesProcessed: totalFilesProcessed,
        filesSkipped: totalFilesSkipped,
        responsesMigrated: totalResponsesMigrated,
        totalFilesAnalyzed: totalFilesProcessed + totalFilesSkipped,
      },
      skippedFiles: skippedFiles,
      filters: {
        skipLargeFiles: skipLargeFiles === "true",
        maxResponses: maxResponsesLimit,
        skippedRounds: roundsToSkip,
      },
      recommendations:
        skippedFiles.length > 0
          ? [
              "Considere processar arquivos grandes em um banco com mais espa√ßo",
              "Use par√¢metros mais restritivos se necess√°rio",
              "Monitore o uso de espa√ßo durante a migra√ß√£o",
            ]
          : [],
    })
  } catch (error) {
    console.error("‚ùå Erro durante a migra√ß√£o [f2f]:", error)
    res.status(500).json({
      success: false,
      error: "Erro interno no servidor durante a migra√ß√£o.",
      details: error.message,
    })
  }
})

// NOVO: Rota para analisar arquivos F2F antes da migra√ß√£o
// GET /api/migration/analyze-f2f-files?maxResponses=15000
router.get("/analyze-f2f-files", ensureServiceInitialized, async (req, res) => {
  try {
    const { maxResponses = "15000" } = req.query
    const maxResponsesLimit = Number.parseInt(maxResponses)

    console.log("üîç Analisando arquivos F2F para migra√ß√£o...")
    console.log(`üìä Limite de respostas: ${maxResponsesLimit.toLocaleString()}`)

    const allFilesByYear = await driveService.listAllF2FSurveyFiles()
    const analysis = {
      totalFiles: 0,
      smallFiles: [],
      largeFiles: [],
      errorFiles: [],
      summary: {
        totalSmallFiles: 0,
        totalLargeFiles: 0,
        totalErrorFiles: 0,
        estimatedSmallResponses: 0,
        estimatedLargeResponses: 0,
      },
    }

    for (const year of Object.keys(allFilesByYear.years)) {
      const yearData = allFilesByYear.years[year]
      console.log(`üìÖ Analisando ano: ${year} (${yearData.files.length} arquivos)`)

      for (const file of yearData.files) {
        analysis.totalFiles++

        try {
          console.log(`üîç Analisando: ${file.name}`)

          const fileData = await driveService.readGoogleSheetsFile(file.id)

          let totalRows = 0
          const sheetDetails = {}

          for (const sheetName of Object.keys(fileData.sheets)) {
            const sheetData = fileData.sheets[sheetName]
            if (sheetData && sheetData.length > 1) {
              const rows = sheetData.length - 1 // -1 para descontar header
              totalRows += rows
              sheetDetails[sheetName] = rows
            }
          }

          const fileInfo = {
            name: file.name,
            year: year,
            rodada: file.rodada,
            totalResponses: totalRows,
            sheets: sheetDetails,
            size: totalRows > maxResponsesLimit ? "LARGE" : "SMALL",
          }

          if (totalRows > maxResponsesLimit) {
            analysis.largeFiles.push(fileInfo)
            analysis.summary.totalLargeFiles++
            analysis.summary.estimatedLargeResponses += totalRows
          } else {
            analysis.smallFiles.push(fileInfo)
            analysis.summary.totalSmallFiles++
            analysis.summary.estimatedSmallResponses += totalRows
          }

          console.log(`   üìä ${totalRows.toLocaleString()} respostas - ${fileInfo.size}`)
        } catch (error) {
          console.error(`‚ùå Erro ao analisar ${file.name}:`, error.message)

          analysis.errorFiles.push({
            name: file.name,
            year: year,
            rodada: file.rodada,
            error: error.message,
          })
          analysis.summary.totalErrorFiles++
        }
      }
    }

    // Ordenar por tamanho
    analysis.largeFiles.sort((a, b) => b.totalResponses - a.totalResponses)
    analysis.smallFiles.sort((a, b) => b.totalResponses - a.totalResponses)

    console.log(`‚úÖ An√°lise conclu√≠da:`)
    console.log(`   üìÅ Total de arquivos: ${analysis.totalFiles}`)
    console.log(`   ‚úÖ Arquivos pequenos: ${analysis.summary.totalSmallFiles}`)
    console.log(`   üö´ Arquivos grandes: ${analysis.summary.totalLargeFiles}`)
    console.log(`   ‚ùå Arquivos com erro: ${analysis.summary.totalErrorFiles}`)

    res.json({
      success: true,
      analysis: analysis,
      recommendations: {
        message: `Encontrados ${analysis.summary.totalLargeFiles} arquivos grandes que devem ser pulados`,
        suggestedCommand: `/api/migration/sync-f2f-surveys?skipLargeFiles=true&maxResponses=${maxResponses}`,
        largeFilesRounds: [...new Set(analysis.largeFiles.map((f) => f.rodada))].filter(Boolean),
        estimatedMigrationSize: `${analysis.summary.estimatedSmallResponses.toLocaleString()} respostas`,
      },
    })
  } catch (error) {
    console.error("‚ùå Erro durante an√°lise:", error)
    res.status(500).json({
      success: false,
      error: "Erro ao analisar arquivos F2F",
      details: error.message,
    })
  }
})

// NOVO: Rota para migrar dados da collection 'test' para 'f2f'
// GET /api/migration/migrate-test-to-f2f?dryRun=true&deleteTest=false&skipExisting=false
router.get("/migrate-test-to-f2f", async (req, res) => {
  try {
    const { dryRun = "true", deleteTest = "false", skipExisting = "false" } = req.query

    console.log("üöÄ Iniciando migra√ß√£o de dados de 'test.responses' para 'f2f.responses'...")
    console.log(`üìä Configura√ß√µes:`)
    console.log(`   - Modo simula√ß√£o: ${dryRun}`)
    console.log(`   - Deletar collection test ap√≥s migra√ß√£o: ${deleteTest}`)
    console.log(`   - Pular documentos j√° existentes: ${skipExisting}`)

    // Conectar ao banco TEST (origem)
    const TestResponse = await getModel("Response", "test")

    // Conectar ao banco f2f (destino)
    const F2FResponse = await getModel("Response", "f2f")
    const F2FSurvey = await getModel("Survey", "f2f")

    console.log("üìä Contando documentos na collection 'test.responses'...")
    const totalDocs = await TestResponse.countDocuments()
    console.log(`   Encontrados ${totalDocs} documentos na origem`)

    // Se skipExisting=true, verificar quantos j√° foram migrados
    let alreadyMigratedIds = new Set()
    let alreadyMigratedCount = 0

    if (skipExisting === "true") {
      console.log("üîç Verificando documentos j√° migrados...")

      // Buscar todos os _id dos documentos j√° migrados no f2f
      // Assumindo que os _id s√£o preservados durante a migra√ß√£o
      const migratedDocs = await F2FResponse.find({}, { _id: 1 }).lean()
      alreadyMigratedIds = new Set(migratedDocs.map((doc) => doc._id.toString()))
      alreadyMigratedCount = alreadyMigratedIds.size

      console.log(`   ‚úÖ ${alreadyMigratedCount} documentos j√° migrados`)
      console.log(`   üìù ${totalDocs - alreadyMigratedCount} documentos restantes para migrar`)
    }

    const docsToMigrate = skipExisting === "true" ? totalDocs - alreadyMigratedCount : totalDocs

    if (totalDocs === 0) {
      return res.json({
        success: false,
        message: "Nenhum documento encontrado na collection 'test.responses'",
        statistics: { totalFound: 0 },
      })
    }

    if (skipExisting === "true" && docsToMigrate === 0) {
      return res.json({
        success: true,
        message: "Todos os documentos j√° foram migrados!",
        statistics: {
          totalDocuments: totalDocs,
          alreadyMigrated: alreadyMigratedCount,
          remaining: 0,
        },
      })
    }

    // Se for dry run, buscar apenas amostra
    if (dryRun === "true") {
      console.log("üì• Buscando amostra de documentos para an√°lise...")
      const sampleDocs = await TestResponse.find({}).limit(10).lean()

      let sampleDoc = null
      let fields = []
      if (sampleDocs.length > 0) {
        sampleDoc = sampleDocs[0]
        fields = Object.keys(sampleDoc)
        console.log("üìã Campos encontrados:", fields.join(", "))
      }

      return res.json({
        success: true,
        message: "An√°lise dos dados (modo simula√ß√£o)",
        isDryRun: true,
        statistics: {
          totalDocuments: totalDocs,
          alreadyMigrated: skipExisting === "true" ? alreadyMigratedCount : 0,
          documentsToMigrate: docsToMigrate,
          fields: fields,
        },
        sampleDocument: sampleDoc,
        nextStep: "Execute com ?dryRun=false para iniciar a migra√ß√£o real",
        warning: `A migra√ß√£o ir√° processar ${docsToMigrate.toLocaleString()} documentos em lotes de 1000`,
        tip: skipExisting === "true" ? "Modo skipExisting ativado - apenas novos documentos ser√£o migrados" : "Use &skipExisting=true para pular documentos j√° migrados",
      })
    }

    // Migra√ß√£o real - processar em lotes com cursor para n√£o sobrecarregar mem√≥ria
    console.log("\nüíæ Iniciando migra√ß√£o em lotes (processamento com cursor)...")
    const batchSize = 1000
    let processedCount = 0
    let skippedCount = 0
    let insertedCount = 0
    let errorCount = 0
    let invalidCount = 0
    const invalidDocs = []

    // Usar cursor para processar documentos em lotes sem carregar tudo na mem√≥ria
    const cursor = TestResponse.find({}).lean().cursor({ batchSize: batchSize })

    let batch = []
    let batchNum = 0

    for await (const doc of cursor) {
      processedCount++

      // Se skipExisting=true, verificar se o documento j√° foi migrado
      if (skipExisting === "true" && alreadyMigratedIds.has(doc._id.toString())) {
        skippedCount++
        if (processedCount % 1000 === 0) {
          console.log(`   ‚è≠Ô∏è  Progresso: ${processedCount}/${totalDocs} processados (${skippedCount} pulados)`)
        }
        continue
      }

      try {
        // Verificar se o documento tem os campos necess√°rios
        if (!doc.surveyId && !doc.surveyName) {
          throw new Error("Documento sem surveyId ou surveyName")
        }

        // Se tiver surveyName mas n√£o tiver surveyId, buscar ou criar a survey no banco f2f
        let surveyId = doc.surveyId
        if (!surveyId && doc.surveyName) {
          const survey = await F2FSurvey.findOneAndUpdate(
            { name: doc.surveyName },
            {
              $set: {
                name: doc.surveyName,
                year: doc.year || new Date().getFullYear(),
                month: doc.rodada || doc.month,
              },
            },
            { upsert: true, new: true },
          )
          surveyId = survey._id
        }

        // Criar o documento no formato Response
        const responseDoc = {
          surveyId: surveyId,
          entrevistadoId: doc.entrevistadoId || doc.respondentId || `resp_${processedCount}`,
          answers: doc.answers || [],
          rodada: doc.rodada || null,
          year: doc.year || new Date().getFullYear(),
        }

        // Validar que tem pelo menos um answer
        if (!responseDoc.answers || responseDoc.answers.length === 0) {
          throw new Error("Documento sem respostas (answers)")
        }

        batch.push(responseDoc)
      } catch (error) {
        invalidCount++
        if (invalidDocs.length < 10) {
          invalidDocs.push({
            docId: doc._id,
            error: error.message,
          })
        }
      }

      // Quando o lote atingir o tamanho desejado, inserir no banco
      if (batch.length >= batchSize) {
        batchNum++
        const totalBatches = Math.ceil(totalDocs / batchSize)

        try {
          console.log(
            `   üì¶ Inserindo lote ${batchNum}/${totalBatches} (${batch.length} documentos) - Processados: ${processedCount}/${totalDocs}`,
          )
          const result = await F2FResponse.insertMany(batch, { ordered: false })
          insertedCount += result.length
          console.log(`      ‚úÖ ${result.length} documentos inseridos`)
        } catch (error) {
          console.log(`      ‚ö†Ô∏è  Erro no lote: ${error.message}`)

          // Tentar inserir um por um para identificar quais falharam
          for (const docToInsert of batch) {
            try {
              await F2FResponse.create(docToInsert)
              insertedCount++
            } catch (err) {
              errorCount++
            }
          }
        }

        // Limpar o lote
        batch = []
      }
    }

    // Inserir documentos restantes (√∫ltimo lote incompleto)
    if (batch.length > 0) {
      batchNum++
      try {
        console.log(`   üì¶ Inserindo lote final (${batch.length} documentos)...`)
        const result = await F2FResponse.insertMany(batch, { ordered: false })
        insertedCount += result.length
        console.log(`      ‚úÖ ${result.length} documentos inseridos`)
      } catch (error) {
        console.log(`      ‚ö†Ô∏è  Erro no lote final: ${error.message}`)

        for (const docToInsert of batch) {
          try {
            await F2FResponse.create(docToInsert)
            insertedCount++
          } catch (err) {
            errorCount++
          }
        }
      }
    }

    console.log(`\n‚úÖ Processamento conclu√≠do!`)
    console.log(`   üìä Total processado: ${processedCount}`)
    if (skipExisting === "true") {
      console.log(`   ‚è≠Ô∏è  Documentos pulados (j√° existentes): ${skippedCount}`)
    }
    console.log(`   ‚úÖ Documentos inseridos: ${insertedCount}`)
    console.log(`   ‚ö†Ô∏è  Documentos inv√°lidos: ${invalidCount}`)
    console.log(`   ‚ùå Erros de inser√ß√£o: ${errorCount}`)

    // Deletar da collection test se solicitado
    let deletedCount = 0
    if (deleteTest === "true") {
      console.log("\nüóëÔ∏è  Deletando documentos da collection 'test.responses'...")
      const deleteResult = await TestResponse.deleteMany({})
      deletedCount = deleteResult.deletedCount
      console.log(`   ‚úÖ ${deletedCount} documentos deletados da collection 'test.responses'`)
    }

    console.log("\n‚úÖ Migra√ß√£o conclu√≠da!")

    res.json({
      success: true,
      message: "Migra√ß√£o conclu√≠da com sucesso",
      statistics: {
        totalDocuments: totalDocs,
        processedDocuments: processedCount,
        skippedDocuments: skipExisting === "true" ? skippedCount : 0,
        alreadyMigrated: skipExisting === "true" ? alreadyMigratedCount : 0,
        validDocuments: insertedCount,
        invalidDocuments: invalidCount,
        insertedDocuments: insertedCount,
        errorDocuments: errorCount,
        deletedFromTest: deletedCount,
      },
      invalidDocumentsDetails: invalidDocs,
      settings: {
        dryRun: false,
        deleteTest: deleteTest === "true",
        skipExisting: skipExisting === "true",
      },
    })
  } catch (error) {
    console.error("‚ùå Erro durante a migra√ß√£o:", error)
    res.status(500).json({
      success: false,
      error: "Erro interno no servidor durante a migra√ß√£o",
      details: error.message,
    })
  }
})

// NOVO: Rota para continuar migra√ß√£o (migrar apenas restantes)
// GET /api/migration/continue-test-to-f2f
router.get("/continue-test-to-f2f", async (req, res) => {
  try {
    console.log("üîÑ Continuando migra√ß√£o de onde parou...")

    // Redirecionar para a rota principal com skipExisting=true
    const { dryRun = "false", deleteTest = "false" } = req.query

    // Conectar aos bancos para verificar status
    const TestResponse = await getModel("Response", "test")
    const F2FResponse = await getModel("Response", "f2f")

    const totalInTest = await TestResponse.countDocuments()
    const totalInF2F = await F2FResponse.countDocuments()
    const remaining = totalInTest - totalInF2F

    console.log(`üìä Status atual:`)
    console.log(`   - Total na origem (test): ${totalInTest}`)
    console.log(`   - Total no destino (f2f): ${totalInF2F}`)
    console.log(`   - Restantes para migrar: ${remaining}`)

    if (remaining <= 0) {
      return res.json({
        success: true,
        message: "Migra√ß√£o j√° est√° completa! Todos os documentos foram migrados.",
        statistics: {
          totalInTest: totalInTest,
          totalInF2F: totalInF2F,
          remaining: 0,
        },
      })
    }

    // Se for dry run, apenas mostrar estat√≠sticas
    if (dryRun === "true") {
      return res.json({
        success: true,
        message: "Status da migra√ß√£o (modo simula√ß√£o)",
        isDryRun: true,
        statistics: {
          totalInTest: totalInTest,
          totalInF2F: totalInF2F,
          remaining: remaining,
        },
        nextStep: `Execute GET /api/migration/migrate-test-to-f2f?dryRun=false&skipExisting=true para continuar`,
        info: "A migra√ß√£o continuar√° de onde parou, pulando os documentos j√° migrados",
      })
    }

    // Executar migra√ß√£o com skipExisting=true
    console.log(`\nüöÄ Iniciando continua√ß√£o da migra√ß√£o...`)
    console.log(`   Redirecionando para migra√ß√£o com skipExisting=true\n`)

    // Chamar a fun√ß√£o de migra√ß√£o interna (n√£o vou duplicar c√≥digo)
    // Redirecionar para a rota principal
    return res.redirect(
      `/api/migration/migrate-test-to-f2f?dryRun=false&skipExisting=true&deleteTest=${deleteTest}`,
    )
  } catch (error) {
    console.error("‚ùå Erro ao continuar migra√ß√£o:", error)
    res.status(500).json({
      success: false,
      error: "Erro ao verificar status da migra√ß√£o",
      details: error.message,
    })
  }
})

// NOVO: Rota para analisar dados da collection 'test'
// GET /api/migration/analyze-test
router.get("/analyze-test", async (req, res) => {
  try {
    console.log("üîç Analisando dados da collection 'test.responses'...")

    // Conectar ao banco TEST (terceiro banco de dados)
    const TestResponse = await getModel("Response", "test")

    console.log("üìä Contando documentos na collection 'test.responses'...")
    const totalDocs = await TestResponse.countDocuments()
    console.log(`   Encontrados ${totalDocs} documentos`)

    if (totalDocs === 0) {
      return res.json({
        success: true,
        message: "Collection 'test.responses' est√° vazia",
        statistics: { totalDocuments: 0 },
      })
    }

    // Buscar alguns documentos de exemplo
    const sampleDocs = await TestResponse.find({}).limit(5).lean()

    // Analisar campos
    const fieldCounts = {}
    const allFields = new Set()

    for (const doc of sampleDocs) {
      for (const field of Object.keys(doc)) {
        allFields.add(field)
        fieldCounts[field] = (fieldCounts[field] || 0) + 1
      }
    }

    // Verificar estrutura
    const structureAnalysis = {
      hasSurveyId: sampleDocs.some((doc) => doc.surveyId),
      hasSurveyName: sampleDocs.some((doc) => doc.surveyName),
      hasAnswers: sampleDocs.some((doc) => doc.answers && doc.answers.length > 0),
      hasEntrevistadoId: sampleDocs.some((doc) => doc.entrevistadoId),
      hasRodada: sampleDocs.some((doc) => doc.rodada),
      hasYear: sampleDocs.some((doc) => doc.year),
    }

    console.log("‚úÖ An√°lise conclu√≠da")

    res.json({
      success: true,
      message: "An√°lise da collection 'test' conclu√≠da",
      statistics: {
        totalDocuments: totalDocs,
        samplesAnalyzed: sampleDocs.length,
        uniqueFields: Array.from(allFields),
        fieldOccurrences: fieldCounts,
      },
      structureAnalysis,
      sampleDocuments: sampleDocs.slice(0, 2).map((doc) => ({
        ...doc,
        _id: doc._id.toString(),
      })),
      recommendations: [
        structureAnalysis.hasSurveyId || structureAnalysis.hasSurveyName
          ? "‚úÖ Documentos t√™m identifica√ß√£o de pesquisa"
          : "‚ö†Ô∏è Documentos n√£o t√™m surveyId ou surveyName",
        structureAnalysis.hasAnswers ? "‚úÖ Documentos t√™m campo answers" : "‚ö†Ô∏è Documentos n√£o t√™m campo answers",
        structureAnalysis.hasEntrevistadoId
          ? "‚úÖ Documentos t√™m entrevistadoId"
          : "‚ö†Ô∏è Documentos n√£o t√™m entrevistadoId",
      ],
      nextSteps: [
        "1. Verifique se a estrutura dos dados est√° correta",
        "2. Execute a migra√ß√£o em modo teste: GET /api/migration/migrate-test-to-f2f?dryRun=true",
        "3. Execute a migra√ß√£o real: GET /api/migration/migrate-test-to-f2f?dryRun=false",
        "4. (Opcional) Delete os dados da collection test: adicione &deleteTest=true",
      ],
    })
  } catch (error) {
    console.error("‚ùå Erro durante an√°lise:", error)
    res.status(500).json({
      success: false,
      error: "Erro ao analisar collection 'test'",
      details: error.message,
    })
  }
})

module.exports = router
