// scripts/export-databases.js
require("dotenv").config()
const fs = require("fs")
const path = require("path")
const archiver = require("archiver")
const { getModel, connectToDatabase } = require("../config/dbManager")

/**
 * Script para exportar databases MongoDB para formatos tabulares (CSV e Parquet)
 *
 * Exporta:
 * - Database F2F: responses, surveys, questionindexes
 * - Database Telephonic: responses, surveys, questionindexes
 * - Database Test: responses
 *
 * Formatos de sa√≠da:
 * - CSV (f√°cil leitura em Excel/Google Sheets)
 * - Parquet (formato otimizado para an√°lise de dados)
 * - JSON (backup completo)
 *
 * Tudo compactado em um arquivo ZIP
 */

// Configura√ß√µes
const EXPORT_DIR = path.join(__dirname, "..", "exports")
const BATCH_SIZE = 1000 // Processar em lotes para n√£o sobrecarregar mem√≥ria

// Criar diret√≥rio de exporta√ß√£o se n√£o existir
if (!fs.existsSync(EXPORT_DIR)) {
  fs.mkdirSync(EXPORT_DIR, { recursive: true })
}

/**
 * Converte array de respostas (formato MongoDB) para colunas (formato tabular)
 */
function flattenResponses(responses) {
  return responses.map((response) => {
    const flat = {
      _id: response._id.toString(),
      surveyId: response.surveyId ? response.surveyId.toString() : null,
      entrevistadoId: response.entrevistadoId,
      rodada: response.rodada,
      year: response.year,
      createdAt: response.createdAt,
      updatedAt: response.updatedAt,
    }

    // Adicionar cada answer como uma coluna
    if (response.answers && Array.isArray(response.answers)) {
      response.answers.forEach((answer) => {
        if (answer.k) {
          flat[answer.k] = answer.v
        }
      })
    }

    return flat
  })
}

/**
 * Exporta collection para CSV usando streaming (sem carregar tudo na mem√≥ria)
 */
async function exportToCSVStream(Model, filename, flatten = false) {
  try {
    console.log(`   üìù Criando CSV: ${filename}`)

    const filepath = path.join(EXPORT_DIR, filename)
    const writeStream = fs.createWriteStream(filepath, { encoding: "utf8" })

    let isFirstRow = true
    let headers = []
    let count = 0

    const cursor = Model.find({}).lean().cursor({ batchSize: BATCH_SIZE })

    for await (const doc of cursor) {
      const processedDoc = flatten ? flattenResponses([doc])[0] : doc

      // Primeira linha: headers
      if (isFirstRow) {
        headers = Object.keys(processedDoc)
        writeStream.write(headers.join(",") + "\n")
        isFirstRow = false
      }

      // Escrever linha de dados
      const values = headers.map((header) => {
        let value = processedDoc[header]
        if (value === null || value === undefined) return ""
        // Escapar v√≠rgulas e aspas
        if (typeof value === "string") {
          value = value.replace(/"/g, '""')
          if (value.includes(",") || value.includes("\n")) {
            value = `"${value}"`
          }
        }
        return value
      })
      writeStream.write(values.join(",") + "\n")

      count++
      if (count % 10000 === 0) {
        console.log(`      Processados ${count.toLocaleString()} documentos...`)
      }
    }

    writeStream.end()

    await new Promise((resolve, reject) => {
      writeStream.on("finish", resolve)
      writeStream.on("error", reject)
    })

    console.log(`   ‚úÖ CSV criado: ${filename} (${count.toLocaleString()} registros)`)
    return filepath
  } catch (error) {
    console.error(`   ‚ùå Erro ao criar CSV ${filename}:`, error.message)
    return null
  }
}

/**
 * Exporta collection para JSON usando streaming
 */
async function exportToJSONStream(Model, filename) {
  try {
    console.log(`   üìù Criando JSON: ${filename}`)

    const filepath = path.join(EXPORT_DIR, filename)
    const writeStream = fs.createWriteStream(filepath, { encoding: "utf8" })

    writeStream.write("[\n")

    let count = 0
    const cursor = Model.find({}).lean().cursor({ batchSize: BATCH_SIZE })

    for await (const doc of cursor) {
      if (count > 0) writeStream.write(",\n")
      writeStream.write("  " + JSON.stringify(doc))
      count++
      if (count % 10000 === 0) {
        console.log(`      Processados ${count.toLocaleString()} documentos...`)
      }
    }

    writeStream.write("\n]")
    writeStream.end()

    await new Promise((resolve, reject) => {
      writeStream.on("finish", resolve)
      writeStream.on("error", reject)
    })

    console.log(`   ‚úÖ JSON criado: ${filename} (${count.toLocaleString()} registros)`)
    return filepath
  } catch (error) {
    console.error(`   ‚ùå Erro ao criar JSON ${filename}:`, error.message)
    return null
  }
}

/**
 * Exporta uma collection completa usando streaming
 */
async function exportCollection(Model, collectionName, dbName, flatten = false) {
  try {
    console.log(`\nüìä Exportando ${dbName}.${collectionName}...`)

    const totalDocs = await Model.countDocuments()
    console.log(`   Total de documentos: ${totalDocs.toLocaleString()}`)

    if (totalDocs === 0) {
      console.log(`   ‚ö†Ô∏è  Collection vazia, pulando...`)
      return { csv: null, json: null, parquet: null }
    }

    const baseFilename = `${dbName}_${collectionName}`

    // Usar streaming para evitar problemas de mem√≥ria
    const csvFile = await exportToCSVStream(Model, `${baseFilename}.csv`, flatten)
    const jsonFile = await exportToJSONStream(Model, `${baseFilename}.json`)

    // Parquet desabilitado por enquanto (causa problemas de mem√≥ria)
    const parquetFile = null

    return { csv: csvFile, json: jsonFile, parquet: parquetFile }
  } catch (error) {
    console.error(`‚ùå Erro ao exportar ${dbName}.${collectionName}:`, error.message)
    return { csv: null, json: null, parquet: null }
  }
}

/**
 * Cria arquivo ZIP com todos os exports
 */
async function createZipArchive(files) {
  return new Promise((resolve, reject) => {
    const timestamp = new Date().toISOString().replace(/:/g, "-").split(".")[0]
    const zipFilename = `mongodb_export_${timestamp}.zip`
    const zipPath = path.join(EXPORT_DIR, zipFilename)

    const output = fs.createWriteStream(zipPath)
    const archive = archiver("zip", {
      zlib: { level: 9 }, // M√°xima compress√£o
    })

    output.on("close", () => {
      const sizeInMB = (archive.pointer() / 1024 / 1024).toFixed(2)
      console.log(`\nüì¶ Arquivo ZIP criado: ${zipFilename}`)
      console.log(`   Tamanho: ${sizeInMB} MB`)
      console.log(`   Local: ${zipPath}`)
      resolve(zipPath)
    })

    archive.on("error", (err) => {
      reject(err)
    })

    archive.pipe(output)

    // Adicionar arquivos ao ZIP
    files.forEach((file) => {
      if (file && fs.existsSync(file)) {
        archive.file(file, { name: path.basename(file) })
      }
    })

    // Criar README dentro do ZIP
    const readme = `
# MongoDB Export - Databases F2F e Telephonic

Exporta√ß√£o realizada em: ${new Date().toISOString()}

## Estrutura dos Arquivos

### Formatos Dispon√≠veis:

1. **CSV** - Formato tabular para Excel/Google Sheets
   - F√°cil de abrir e visualizar
   - Todas as respostas s√£o colunas separadas

2. **JSON** - Formato completo (backup)
   - Mant√©m estrutura original do MongoDB
   - √ötil para re-importa√ß√£o

3. **Parquet** - Formato otimizado para an√°lise
   - Apenas para responses (devido ao tamanho)
   - Ideal para Python/Pandas, R, Spark, etc.

### Databases Exportados:

- **f2f**: Pesquisas Face-to-Face
- **telephonic**: Pesquisas Telef√¥nicas
- **test**: Dados de teste

### Collections:

- **responses**: Respostas dos entrevistados
- **surveys**: Informa√ß√µes das pesquisas
- **questionindexes**: √çndice de perguntas

## Como Usar

### CSV (Excel/Google Sheets):
1. Abra o arquivo .csv com Excel ou Google Sheets
2. Os dados j√° est√£o em formato tabular

### JSON (Programa√ß√£o):
\`\`\`javascript
const data = require('./f2f_responses.json')
console.log(data)
\`\`\`

### Parquet (Python/Pandas):
\`\`\`python
import pandas as pd
df = pd.read_parquet('f2f_responses.parquet')
print(df.head())
\`\`\`

## Notas

- Responses est√£o "achatados" (flatten) nos formatos CSV e Parquet
- Cada resposta (answer.k) virou uma coluna
- O formato JSON mant√©m a estrutura original aninhada
`

    archive.append(readme, { name: "README.txt" })

    archive.finalize()
  })
}

/**
 * Fun√ß√£o principal de exporta√ß√£o
 */
async function exportAllDatabases() {
  console.log("üöÄ Iniciando exporta√ß√£o das databases MongoDB...\n")

  const allFiles = []

  try {
    // ============================================
    // EXPORTAR DATABASE F2F
    // ============================================
    console.log("=" .repeat(60))
    console.log("üìÅ DATABASE: F2F")
    console.log("=".repeat(60))

    const F2FResponse = await getModel("Response", "f2f")
    const F2FSurvey = await getModel("Survey", "f2f")
    const F2FQuestionIndex = await getModel("QuestionIndex", "f2f")

    const f2fResponses = await exportCollection(F2FResponse, "responses", "f2f", true)
    const f2fSurveys = await exportCollection(F2FSurvey, "surveys", "f2f", false)
    const f2fQuestions = await exportCollection(F2FQuestionIndex, "questionindexes", "f2f", false)

    allFiles.push(f2fResponses.csv, f2fResponses.json, f2fResponses.parquet)
    allFiles.push(f2fSurveys.csv, f2fSurveys.json)
    allFiles.push(f2fQuestions.csv, f2fQuestions.json)

    // ============================================
    // EXPORTAR DATABASE TELEPHONIC
    // ============================================
    console.log("\n" + "=".repeat(60))
    console.log("üìÅ DATABASE: TELEPHONIC")
    console.log("=".repeat(60))

    const TelResponse = await getModel("Response", "telephonic")
    const TelSurvey = await getModel("Survey", "telephonic")
    const TelQuestionIndex = await getModel("QuestionIndex", "telephonic")

    const telResponses = await exportCollection(TelResponse, "responses", "telephonic", true)
    const telSurveys = await exportCollection(TelSurvey, "surveys", "telephonic", false)
    const telQuestions = await exportCollection(TelQuestionIndex, "questionindexes", "telephonic", false)

    allFiles.push(telResponses.csv, telResponses.json, telResponses.parquet)
    allFiles.push(telSurveys.csv, telSurveys.json)
    allFiles.push(telQuestions.csv, telQuestions.json)

    // ============================================
    // EXPORTAR DATABASE TEST (opcional)
    // ============================================
    console.log("\n" + "=".repeat(60))
    console.log("üìÅ DATABASE: TEST")
    console.log("=".repeat(60))

    const TestResponse = await getModel("Response", "test")
    const testResponses = await exportCollection(TestResponse, "responses", "test", true)

    allFiles.push(testResponses.csv, testResponses.json, testResponses.parquet)

    // ============================================
    // CRIAR ARQUIVO ZIP
    // ============================================
    console.log("\n" + "=".repeat(60))
    console.log("üì¶ CRIANDO ARQUIVO ZIP")
    console.log("=".repeat(60))

    const validFiles = allFiles.filter(Boolean)
    const zipPath = await createZipArchive(validFiles)

    // ============================================
    // LIMPEZA - Deletar arquivos individuais
    // ============================================
    console.log("\nüßπ Limpando arquivos individuais...")
    validFiles.forEach((file) => {
      try {
        fs.unlinkSync(file)
      } catch (err) {
        // Ignorar erros
      }
    })

    console.log("\n" + "=".repeat(60))
    console.log("‚úÖ EXPORTA√á√ÉO CONCLU√çDA COM SUCESSO!")
    console.log("=".repeat(60))
    console.log(`\nüì¶ Arquivo final: ${zipPath}`)
    console.log(`\nüí° Pr√≥ximos passos:`)
    console.log(`   1. Baixe o arquivo ZIP`)
    console.log(`   2. Extraia os arquivos`)
    console.log(`   3. Use CSV para Excel ou Parquet para an√°lise em Python/R`)
    console.log(`\n`)
  } catch (error) {
    console.error("\n‚ùå Erro durante a exporta√ß√£o:", error)
    throw error
  } finally {
    // Fechar conex√µes
    process.exit(0)
  }
}

// Executar exporta√ß√£o
exportAllDatabases()
  .then(() => {
    console.log("üéâ Script finalizado!")
  })
  .catch((error) => {
    console.error("‚ùå Script finalizado com erro:", error)
    process.exit(1)
  })
